{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Analyzing StockTwitz Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}, {'message_body': '$SBUX STRONG BUY!', 'sentiment': 2, 'timestamp': '2018-07-01T00:09:26Z'}, {'message_body': '$SNPS short ratio is 2.17 at 2018-06-15 and short % to float is 1.42% http://sunshineavenue.com/stock/SNPS/ via @sunshineave', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:36Z'}, {'message_body': '$NFLX price squeezing,perfect place for an option straddle near the supporting trend', 'sentiment': 2, 'timestamp': '2018-07-01T00:12:58Z'}, {'message_body': '@DEEPAKM2013 @Nytunes Start of new Q on Monday. Expect strong buy volume across key companies of various sectors. $AMZN $AAPL', 'sentiment': 2, 'timestamp': '2018-07-01T00:13:57Z'}, {'message_body': '[BREAKOUT Strategy] Current Portfolio : $ZBRA,$WEB,$TIF,$SRE,$SPPI,$OTEX,$OMF,$NVGS,$NRCIB,$MSG,$KIRK,$GHDX,$FBNK,$ESND,$DRI,$DKS,$CVTI,$CV', 'sentiment': 2, 'timestamp': '2018-07-01T00:14:19Z'}, {'message_body': '$PFE bulls have 3 reasons on 06-29 to pay more attention https://dividendbot.com?s=PFE', 'sentiment': 1, 'timestamp': '2018-07-01T00:16:07Z'}, {'message_body': '$AMZN 3 catalysts 4 Continuing this new uptrend; #1- Pill Pack buy out #2- Amazon Prime Day #3- Earnings. Test/break of 1763 soon $SPY $QQQ', 'sentiment': 2, 'timestamp': '2018-07-01T00:19:41Z'}, {'message_body': '$AAPL has moved -0.21% on 06-29. Check out the movement and peers at  https://dividendbot.com?s=AAPL', 'sentiment': 0, 'timestamp': '2018-07-01T00:20:08Z'}, {'message_body': '#STAAnalystAlert for $TGT : MKM Partners Set Price Target with a rating of Buy setting target price at USD 91.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:21:23Z'}, {'message_body': '$DNR I think it is too early to tell what’s going to happen Monday. Even with the current news about raisinf output, there is still so many', 'sentiment': 1, 'timestamp': '2018-07-01T00:22:56Z'}]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "json_object = json.dumps(twits)\n",
    "with open('twits.json','w') as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "print(twits['data'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "total_twits = len(twits['data'])\n",
    "print(total_twits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "The preprocessing steps we will take will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `preprocess` function to remove any characters that doesn't add any sentiment meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "     \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"https?://\\S+\",' ',text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$\\S+\",' ',text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@\\S+\",' ',text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-zA-Z]\",' ',text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "\n",
    "    \n",
    "    assert type(tokens) == list, 'Tokens should be list'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement\n",
    "tokenized = [preprocess(msg) for msg in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text $AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.\n",
      "Tokenized List ['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'on', 'stocktwits']\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Text\", messages[3])\n",
    "print(\"Tokenized List\", tokenized[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Count how often each words appear in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "# TODO: Implement \n",
    "word_tokens = []\n",
    "for token in tokenized:\n",
    "    for word in token:\n",
    "        word_tokens.append(word)\n",
    "counts = Counter(word_tokens)\n",
    "\n",
    "bow = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 398754), ('to', 379487), ('is', 284865), ('for', 273538), ('on', 241663), ('of', 211334), ('and', 208471), ('in', 205307), ('this', 203540), ('it', 193485), ('at', 138453), ('will', 128180), ('up', 121567), ('are', 101424), ('you', 94275), ('that', 89655), ('be', 89277), ('short', 86639), ('what', 79113), ('today', 76240)]\n"
     ]
    }
   ],
   "source": [
    "print(bow.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "Remove most common words such as 'the', 'and', 'it' and also rare words that apepars in a few twits. The purpose of this is to reduce the amount of noise in our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['the', 'to', 'is', 'for', 'on', 'of', 'and', 'in', 'this', 'it', 'at', 'will', 'up', 'are', 'you', 'that', 'be']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14958"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "#print(len(messages) == total_twits)\n",
    "total_word = sum(bow.values()) #Total number of words not total number of twits \n",
    "\n",
    "freqs = {word: cnt/total_word for word, cnt in bow.items()}\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 1e-6\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 17\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = [word[0] for word in bow.most_common(high_cutoff)]\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(K_most_common)\n",
    "len(filtered_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words\n",
    "Let's creat three variables that will help with our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab = {word:i  for i,word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {i:word for word,i in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "assert set(vocab.keys()) == set(id2vocab.values()), 'Check vocab and id2vocab dictionaries'\n",
    "#filtered =  [[word for word in msg if (freqs[word] > low_cutoff and word not in K_most_common)]for msg in tokenized]\n",
    "filtered = [[word for word in msg if word in filtered_words] for msg in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "If we look at how our twits are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/(4*n_neutral)\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the number of neutral twits are reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19396845955927397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "The diagram of the network \n",
    "\n",
    "#### Embed -> LSTM> Dense -> Softmax\n",
    "### Implement the text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,lstm_size,lstm_layers,dropout= dropout, batch_first= True)\n",
    "        \n",
    "        self.fc = nn.Linear(lstm_size,output_size)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        # Setup additional layers\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = nn_input.size(0)\n",
    "        #print(\"Batch Size:\",batch_size)\n",
    "        seq_length = nn_input.size(1)\n",
    "        embedding = self.embedding(nn_input)\n",
    "        \n",
    "        lstm_out, hidden_state = self.lstm(embedding, hidden_state)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        logsoft_out = F.log_softmax(out)\n",
    "        logsoft_out = logsoft_out.view(batch_size,seq_length, -1)\n",
    "        logsoft_out = logsoft_out[:,-1,:]\n",
    "        # TODO Implement \n",
    "        \n",
    "        return logsoft_out, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Build `dataloader` to support batching to generate the current batch inputs and labels. This enable the program to loop through the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "    \n",
    "    total_sequences = len(messages)\n",
    "    n_batch = total_sequences // batch_size\n",
    "    messages = messages[:n_batch * batch_size]\n",
    "    \n",
    "    for ii in range(0, n_batch * batch_size, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((len(batch_messages), sequence_length), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[batch_num, start_idx:] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822715\n",
      "102839\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "split= int(0.8 * len(token_ids))\n",
    "train_features, remain_features = token_ids[:split], token_ids[split:]\n",
    "train_labels, remain_labels = sentiments[:split], sentiments[split:]\n",
    "\n",
    "split = int(0.5 * len(remain_features))\n",
    "valid_features, test_features = remain_features[:split],remain_features[split:]\n",
    "valid_labels, test_labels = remain_labels[:split],remain_labels[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(14959, 512)\n",
       "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.4)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 512, 512, 5, lstm_layers=2, dropout=0.4)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ... Step: 100 ... Loss:0.925 ... Val Loss:0.926 ... Val Accruacy: 0.632\n",
      "Epoch: 1 ... Step: 200 ... Loss:0.854 ... Val Loss:0.846 ... Val Accruacy: 0.672\n",
      "Epoch: 1 ... Step: 300 ... Loss:0.862 ... Val Loss:0.803 ... Val Accruacy: 0.691\n",
      "Epoch: 1 ... Step: 400 ... Loss:0.839 ... Val Loss:0.783 ... Val Accruacy: 0.700\n",
      "Epoch: 1 ... Step: 500 ... Loss:0.735 ... Val Loss:0.775 ... Val Accruacy: 0.691\n",
      "Epoch: 1 ... Step: 600 ... Loss:0.719 ... Val Loss:0.769 ... Val Accruacy: 0.705\n",
      "Epoch: 1 ... Step: 700 ... Loss:0.755 ... Val Loss:0.756 ... Val Accruacy: 0.711\n",
      "Epoch: 1 ... Step: 800 ... Loss:0.762 ... Val Loss:0.749 ... Val Accruacy: 0.713\n",
      "Epoch: 1 ... Step: 900 ... Loss:0.803 ... Val Loss:0.741 ... Val Accruacy: 0.718\n",
      "Epoch: 1 ... Step: 1000 ... Loss:0.761 ... Val Loss:0.738 ... Val Accruacy: 0.718\n",
      "Epoch: 1 ... Step: 1100 ... Loss:0.784 ... Val Loss:0.755 ... Val Accruacy: 0.713\n",
      "Epoch: 1 ... Step: 1200 ... Loss:0.750 ... Val Loss:0.729 ... Val Accruacy: 0.719\n",
      "Epoch: 1 ... Step: 1300 ... Loss:0.787 ... Val Loss:0.731 ... Val Accruacy: 0.720\n",
      "Epoch: 1 ... Step: 1400 ... Loss:0.781 ... Val Loss:0.726 ... Val Accruacy: 0.721\n",
      "Epoch: 1 ... Step: 1500 ... Loss:0.775 ... Val Loss:0.729 ... Val Accruacy: 0.720\n",
      "Epoch: 1 ... Step: 1600 ... Loss:0.706 ... Val Loss:0.723 ... Val Accruacy: 0.722\n",
      "Starting epoch 2\n",
      "Epoch: 2 ... Step: 100 ... Loss:0.692 ... Val Loss:0.728 ... Val Accruacy: 0.722\n",
      "Epoch: 2 ... Step: 200 ... Loss:0.751 ... Val Loss:0.727 ... Val Accruacy: 0.722\n",
      "Epoch: 2 ... Step: 300 ... Loss:0.717 ... Val Loss:0.726 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 400 ... Loss:0.678 ... Val Loss:0.725 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 500 ... Loss:0.708 ... Val Loss:0.725 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 600 ... Loss:0.757 ... Val Loss:0.729 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 700 ... Loss:0.705 ... Val Loss:0.727 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 800 ... Loss:0.767 ... Val Loss:0.727 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 900 ... Loss:0.752 ... Val Loss:0.722 ... Val Accruacy: 0.722\n",
      "Epoch: 2 ... Step: 1000 ... Loss:0.745 ... Val Loss:0.725 ... Val Accruacy: 0.720\n",
      "Epoch: 2 ... Step: 1100 ... Loss:0.719 ... Val Loss:0.719 ... Val Accruacy: 0.724\n",
      "Epoch: 2 ... Step: 1200 ... Loss:0.645 ... Val Loss:0.723 ... Val Accruacy: 0.721\n",
      "Epoch: 2 ... Step: 1300 ... Loss:0.715 ... Val Loss:0.719 ... Val Accruacy: 0.723\n",
      "Epoch: 2 ... Step: 1400 ... Loss:0.706 ... Val Loss:0.720 ... Val Accruacy: 0.724\n",
      "Epoch: 2 ... Step: 1500 ... Loss:0.702 ... Val Loss:0.718 ... Val Accruacy: 0.724\n",
      "Epoch: 2 ... Step: 1600 ... Loss:0.760 ... Val Loss:0.718 ... Val Accruacy: 0.724\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 512\n",
    "learning_rate = 0.005\n",
    "sl = 50\n",
    "print_every = 100\n",
    "clip = 5\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    steps = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for text_batch, labels in dataloader(train_features, train_labels, \\\n",
    "                                         batch_size=batch_size, sequence_length=sl, shuffle=True):\n",
    "        steps += 1\n",
    "        \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        model.zero_grad() #Reset Gradient\n",
    "        log_ps, hidden = model.forward(text_batch, hidden) #Compute Result\n",
    "        loss = criterion(log_ps,labels) #Compute Loss\n",
    "        loss.backward() #Backpropagate\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),clip) #Gradient Clip\n",
    "        optimizer.step() #Update parameters\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            val_accuracy = []\n",
    "            \n",
    "            valid_loader = dataloader(valid_features, valid_labels, batch_size=batch_size, \\\n",
    "                                      sequence_length=sl, shuffle=True)\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            for val_batch, val_labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if train_on_gpu:\n",
    "                    val_batch, val_labels = val_batch.cuda(), val_labels.cuda()\n",
    "                \n",
    "                log_ps, val_h = model(val_batch, val_h)\n",
    "                val_loss = criterion(log_ps, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                val_ps = torch.exp(log_ps)\n",
    "                _,top_class = val_ps.topk(1,dim=1)\n",
    "                \n",
    "                val_accuracy.append(torch.mean(top_class.eq(val_labels.view_as(top_class)).float()).item())\n",
    "            \n",
    "            print(\"Epoch: {} ...\".format(epoch+1),\n",
    "                  \"Step: {} ...\".format(steps),\n",
    "                  \"Loss:{:.3f} ...\".format(loss.item()),\n",
    "                  \"Val Loss:{:.3f} ...\".format(np.mean(val_losses)),\n",
    "                  \"Val Accruacy: {:.3f}\".format(np.mean(val_accuracy)))\n",
    "            \n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction \n",
    "The `predict` function generate the prediction vector from a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in filtered_words]\n",
    "    \n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] for word in tokens]\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "    text_input = text_input.to('cuda')\n",
    "    logps, _ = model.forward(text_input,hidden)\n",
    "    \n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps).to('cpu')\n",
    "    \n",
    "    return pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5.42058668e-04,   1.43642137e-02,   7.90409371e-03,\n",
       "          7.46714711e-01,   2.30474994e-01]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': array([[ 0.12625226,  0.11622299,  0.17909808,  0.30883443,  0.26959223]], dtype=float32),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
